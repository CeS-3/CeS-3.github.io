---
title: 记一次爬虫项目实践
description: 爬取HTB用户信息
categories: [Python, crawl]
tags: [crawler]
image: 
    path: assets/img/bg/pic01.jpg
    lqip: assets/img/bg/pic01.jpg
published: false
---
## 简介
暑假参与NUS Soc，做了个Web Mining课程的[爬虫项目](https://github.com/CeS-3/HTB_scraper)，之前一直懂个皮毛，没怎么用就忘得差不多了，正好借这次机会复习一下相关知识，也见识见识真的爬虫项目的流程。

## 技术要点
1. python request库的使用  
2. python mongo数据库接口的使用
3. python beautifulSoup库的使用
4. Web 结构分析及请求处理原理
5. python json的处理

## 实现过程
### 目标分析
项目要求从[HTB lab](https://app.hackthebox.com/home)网站中爬取用户数据，注意到主要的用户数据集中于[用户的个人界面](https://app.hackthebox.com/users/13569)，因此爬取的目标便是这些用户的个人页面，观察这些个人页面的url：
`https://app.hackthebox.com/users/13569`  
注意到他们的主要是由`https://app.hackthebox.com/users/` + user_id构成的。当然可以直接从0开始遍历整数集作为user_id与url头拼接得到目标url集，但为了提高爬取的用户信息的质量，应该要从[用户的database](https://www.hackthebox.com/players)中爬取user_id。得到高质量的user_id集后再构造对应的url，爬取用户信息。
### 爬取user_id
首先观察[用户的database](https://www.hackthebox.com/players)的源代码，发现所需要的信息(图中用户名下方的user_id)并未储存在源代码中。  
![alt text](/assets/images/scrap/image.png "如user_id 13569 就没有在源代码中出现")  
f12进行抓包，发现当前页面内容是通过API调用获取后再渲染到前端的，其API url的https://labs.hackthebox.com/api/v4/players/fetch_data?page=1 末尾的page={num}为页数，而当前可供访问的页数为1500页，因此从1到1500循环模拟该API的请求。  
再看单个API调用结果是个HTML文件，关键部分的代码如下：  
```HTML
<div class="d-flex flex-column justify-content-center overflow-hidden">
    <p class="font-size15 font-weight500 color-white mb-0">xct</p>
    <p class="font-size13 line-height-18 mb-0"># 13569</p>
</div>
```
使用beuatifulSoup包解析页面，select该元素即可。
实现代码如下
```python
def get_user_id_in_database(page_num,file_path):
    with open(file_path,'w') as file:
        for num in range(1,page_num + 1):
            database_url = f"https://labs.hackthebox.com/api/v4/players/fetch_data?page={num}"
            response = requests.get(url=database_url,headers=headers)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content,"html.parser")
                id_elements = soup.select("p.font-size13.line-height-18.mb-0")
                for id_element in id_elements:
                    file.write(id_element.text.replace("#","").strip() + "\n")
                print(f"第{num}页爬取完成")
            else:
                print(f"请求失败，状态码: {response.status_code}") 
            time.sleep(1)

page_num = 1500

get_user_id_in_database(page_num,file_path)
```
### 爬取用户信息
![alt text](/assets/images/scrap/image-1.png)
![alt text](/assets/images/scrap/image-2.png)
观察页面目标内容，锁定对应url头
```python
url_dict = {
    "graph_url": "https://labs.hackthebox.com/api/v4/user/profile/graph/1Y/",
    "machine_attack_url": "https://labs.hackthebox.com/api/v4/user/profile/chart/machines/attack/",
    "machine_os_url": "https://labs.hackthebox.com/api/v4/user/profile/progress/machines/os/",
    "challenge_url": "https://labs.hackthebox.com/api/v4/user/profile/progress/challenges/",
    "sherlock_url": "https://labs.hackthebox.com/api/v4/user/profile/progress/sherlocks/",
    "user_info_url": "https://labs.hackthebox.com/api/v4/user/profile/basic/"
}
```
使用user_id与url头拼接在一起构成完整url。
```python
full_url = url + user_id
```
然后伪造请求获取内容即可。  
**但注意到此处的请求头带了一个Authorization字段**  
![alt text](/assets/images/scrap/image-3.png "该字段有页面根据用户登录信息动态生成")  
该字段有页面根据用户登录信息动态生成，不带有该字段的请求都会收到401错误码。  
_由于时间紧迫，没有仔细研究其认证机制_，采用手动输入Authorization字段数据。根据实验结果发现一次输入可以使用7-8个小时。  
由于爬取的数据较多、Token值需手动输入、时间紧迫等因素的影响，要求该程序支持重入。  
而该重入机制的关键在于数据的静态存储，即需要将得到的数据存在磁盘中，而不是内存中。  
我首先想到的是使用文件，将得到的响应json字符串存入写入文件中。  
程序运行前先将文件中的数据转化为字典对象，遍历得到的user_id集合，若该id对应的条目已经在该字典对象中了就直接跳过，快速移动到上次运行未爬取的id处。  
但这种处理方式的弊端也很明显：__需要将整个json文件加载到内存的一个对象中，很可能占用过多内存空间。__  
因此采用第二种方案：调用一个数据库接口，将爬取的数据存入数据库，爬取之前先在数据库中搜索对应id对应的条目，若已有则跳过，爬取结束后再将数据库中的数据导出，组织为所需的格式即可。
```python
with open("user_id.txt", "r") as user_ids:
    for user_id in user_ids:
        user_id = user_id.strip()  # 去除换行符和空格
        if db.if_exist(user_id):
            print(f"用户 {user_id} 已存在，pass")
            continue

        user_data = []

        for url in url_dict.values():
            full_url = url + user_id
            while True:
                #发送请求
                response = requests.get(full_url, headers=headers)
                #若请求成功
                if response.status_code == 200:
                    #插入该项
                    data = response.json()
                    user_data.append(data)
                    print(".", end='')
                    #离开该死循环
                    break
                elif response.status_code == 401:
                    #若token过期
                    print("Token过期,换一个")
                    #请求一个新的token
                    get_new_Token()
                    time.sleep(0.75)
                    #此处不离开死循环，再进行一次请求
                else:
                    print(f"被封了,响应码:{response.status_code}")
                    flag = False
                    #若ip被封
                    #直接离开死循环
                    break
            if not flag:
                break 
            time.sleep(0.75)

        if not flag:
            break
        db.insert_user_data(user_id, user_data)
        print(f"用户 {user_id} 爬取完成")
```



